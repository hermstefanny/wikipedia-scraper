{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rru15J6cT0aF"
      },
      "source": [
        "# Your first scraper\n",
        "In this project, we will guide you step by step through the process of:\n",
        "\n",
        "1. creating a self-contained development environment.\n",
        "1. retrieving some information from an API (a website for computers)\n",
        "2. leveraging it to scrape a website that does not provide an API\n",
        "3. saving the output for later processing\n",
        "\n",
        "Here we query an API for a list of countries and their past leaders. We then extract and sanitize their short bio from Wikipedia. Finally, we save the data to disk.\n",
        "\n",
        "This task is often the first (coding) step of a datascience project and you will often come back to it in the future.\n",
        "\n",
        "You will study topics such as *scraping*, *data structures*, *regular expressions*, *concurrency* and *file handling*. We will point out useful resources at the appropriate time. \n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Creating a clean environment\n",
        "\n",
        "Use the [`venv`](https://docs.python.org/3/library/venv.html) command to create a new environment called `wikipedia_scraper_env`.\n",
        "\n",
        "Activate it and add it to you `.gitignore` file. \n",
        "\n",
        "You will find more info about virtual environments in the course content and on the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qvKH56a1Wy"
      },
      "source": [
        "## 1. API Scraping\n",
        "\n",
        "### 1a. A simple API query\n",
        "You will start with the basics: how to do a simple request to an [API endpoint](../../2.python/2.python_advanced/05.Scraping/5.apis.ipynb).\n",
        "\n",
        "You will use the [requests](https://requests.readthedocs.io/en/latest/) external library through the `import` keyword. NOTE: external libraries need to be installed first. Check the [request Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) section of the documentation to:\n",
        "\n",
        "1. Use the `get()` method to connect to this endpoint: https://country-leaders.onrender.com/status\n",
        "2. Check if the `status_code` is equal to 200, which means OK.\n",
        "    * if OK, `print()` the `text`` of the response.\n",
        "    * if not, `print()` the `status_code`. \n",
        "\n",
        "Here is an explanation of [HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9baoMWgIcK3E"
      },
      "outputs": [],
      "source": [
        "# import the requests library (1 line)\n",
        "import requests\n",
        "\n",
        "# assign the root url (without /status) to the root_url variable for ease of reference (1 line)\n",
        "root_url = \"https://country-leaders.onrender.com/\"\n",
        "\n",
        "# assign the /status endpoint to another variable called status_url (1 line)\n",
        "status_url =\"https://country-leaders.onrender.com/status/\"\n",
        "\n",
        "# query the /status endpoint using the get() method and store it in the req variable (1 line)\n",
        "response = requests.get(status_url)\n",
        "\n",
        "# check the status_code using a condition and print appropriate messages (4 lines)\n",
        "response_code = response.status_code\n",
        "if response_code == 200:\n",
        "    print(f\"{response_code}: OK, request succesful\")\n",
        "else:\n",
        "    print(\"ERROR. Something went wrong\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL72ovJUcZTA"
      },
      "source": [
        "### 1b. Dealing with JSON\n",
        "\n",
        "[JSON](https://quickref.me/json) is the preferred format to deal with data over the web. You cannot avoid it so you would better get acquainted.\n",
        "\n",
        "Connect to another endpoint called `/countries` but this time the API will return data in the JSON format. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0DLiYCWcg5W"
      },
      "outputs": [],
      "source": [
        "# Set the countries_url variable (1 line)\n",
        "countries_url = root_url+ \"countries\"\n",
        "\n",
        "# query the /countries endpoint using the get() method and store it in the req variable (1 line)\n",
        "countries_resp = requests.get(countries_url)\n",
        "\n",
        "# Get the JSON content and store it in the countries variable (1 line)\n",
        "print(countries_resp.json)\n",
        "\n",
        "# display the request's status code and the countries variable (1 line)\n",
        "print(countries_resp.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x25JA6vaRBi"
      },
      "source": [
        "### 1c. Cookies anyone?\n",
        "\n",
        "It looks like the access to this API is restricted...\n",
        "Query the `/cookie` endpoint and extract the appropriate field to access your cookie.\n",
        "\n",
        "You will need to use this cookie in each of the following API requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTDwpN9Q3nk_"
      },
      "outputs": [],
      "source": [
        "# Set the cookie_url variable (1 line)\n",
        "cookie_url = root_url + \"cookie/\"\n",
        "\n",
        "# Query the enpoint, set the cookies variable and display it (2 lines)\n",
        "cookie_resp = requests.get(cookie_url)\n",
        "user_cookie = cookie_resp.cookies['user_cookie']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBNaFJo2M9e"
      },
      "source": [
        "Try to query the countries endpoint using the cookie, save the output and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_user_cookie():\n",
        "    cookie_url = root_url + \"cookie/\"\n",
        "    cookie_resp = requests.get(cookie_url)\n",
        "    return {'user_cookie': cookie_resp.cookies['user_cookie']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y63sTXY7ppT"
      },
      "outputs": [],
      "source": [
        "# query the /countries endpoint, assign the output to the countries variable (1 line)\n",
        "\n",
        "countries_resp = requests.get(countries_url, cookies=call_user_cookie())\n",
        "\n",
        "# display the countries variable (1 line)\n",
        "print(countries_resp.json)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "countries = countries_resp.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HmuGTT9lU1"
      },
      "source": [
        "Chances are the cookie has expired... Thankfully, you got a nice error message. For now, simply execute the last 2 cells quickly so you get a result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egv40GBV8rSH"
      },
      "source": [
        "### 1d. Getting the actual data from the API\n",
        "\n",
        "Query the `/leaders` endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwLFqcBA8PaD"
      },
      "outputs": [],
      "source": [
        "# Set the leaders_url variable (1 line)\n",
        "leaders_url = root_url+\"/leaders\"\n",
        "# query the /leaders endpoint, assign the output to the leaders variable (1 line)\n",
        "leaders_resp = requests.get(leaders_url, cookies=call_user_cookie())\n",
        "\n",
        "# display the leaders variable (1 line)\n",
        "print(leaders_resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QyM7vWBAlY4"
      },
      "source": [
        "It looks like this endpoint requires additional information in order to return its result. Check the API [*documentation*](https://country-leaders.onrender.com/docs) in your web browser.\n",
        "\n",
        "Change the query to accept *parameters*. You should know where to find help by now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIEFBhBeAkzf"
      },
      "outputs": [],
      "source": [
        "# query the /leaders endpoint using cookies and parameters (take any country in countries)\n",
        "# assign the output to the leaders variable (1 line)\n",
        "leaders_resp = requests.get(leaders_url, cookies=call_user_cookie(), params={'country':'be'})\n",
        "# display the leaders variable (1 line)\n",
        "print(leaders_resp.content)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uW3k5uquCirA"
      },
      "source": [
        "### 1e. A sneak peak at the data (finally)\n",
        "\n",
        "Look inside a few examples. Notice the dictionary keys available for each entry. You have your first example of *structured data*. This data was sanitized for your benefit, meaning it is readily exploitable without modification.\n",
        "\n",
        "You will also notice there is a Wikipedia link for each entry. You will need to extract additional information there. This will be a case of *semi-structured* data.\n",
        "\n",
        "The /countries endpoint returns a `list` of several country codes.\n",
        "\n",
        "You need to loop through this list and query the /leaders endpoint for each one. Save each `json` result in a dictionary called `leaders_per_country`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8EdK2S9rvPJ"
      },
      "outputs": [],
      "source": [
        "# 4 lines\n",
        "leaders_per_country =dict()\n",
        "for country in countries:\n",
        "    country_resp = requests.get(leaders_url, cookies=call_user_cookie(), params={'country':country})\n",
        "    leaders_per_country[country]=country_resp.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in leaders_per_country.items():\n",
        "    print(f\"For country {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsA2j7s_RMgy"
      },
      "source": [
        "It is finally time to create a `get_leaders()` function for the above code. You will build on it later-on. This function takes no parameter. Inside it, you will need to:\n",
        "1. define the urls\n",
        "2. get the cookies\n",
        "2. get the countries\n",
        "3. loop over them and save their leaders in a dictionary\n",
        "4. return the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6C7BgWQMxV2"
      },
      "outputs": [],
      "source": [
        "# < 15 lines\n",
        "def get_leaders():\n",
        "    # Establishing the urls\n",
        "    root_url = \"https://country-leaders.onrender.com/\"\n",
        "    cookie_url = root_url+\"cookie/\"\n",
        "    countries_url = root_url + \"countries\"\n",
        "    leaders_url = root_url+\"/leaders\"\n",
        "    leaders_per_country =dict()\n",
        "\n",
        "    # Getting the cookie\n",
        "    cookie_resp = requests.get(cookie_url)\n",
        "    user_cookie = cookie_resp.cookies['user_cookie']\n",
        "\n",
        "    countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "    countries = countries_resp.json()\n",
        "    \n",
        "    for country in countries:\n",
        "        \n",
        "        country_resp = requests.get(leaders_url, cookies={'user_cookie':user_cookie}, params={'country':country})\n",
        "        leaders_per_country[country]=country_resp.text\n",
        "    \n",
        "    return leaders_per_country\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I76-InoKuuV8"
      },
      "source": [
        "Test your function, save the result in the `leaders_per_country` dictionary and check its ouput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXwd8o7Gu8yG"
      },
      "outputs": [],
      "source": [
        "# 2 lines\n",
        "leaders_p_country = get_leaders()\n",
        "\n",
        "for k, v in leaders_p_country.items():\n",
        "    print(f\"For country {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## To do\n",
        "### Function to extract the wikipedia_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Fc1mHySn9g"
      },
      "source": [
        "## 2. Extracting data from Wikipedia\n",
        "\n",
        "Query one of the leaders' Wikipedia urls and display its `text` (not JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEKKqyTHr3fD"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "dict_us = leaders_p_country['us']\n",
        "dict_ma = leaders_p_country['ma']\n",
        "dict_fr = leaders_p_country['fr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(dict_us)\n",
        "print(dict_ma)\n",
        "print(dict_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert string to dictionary\n",
        "ma_json = json.loads(dict_ma)\n",
        "fr_json = json.loads(dict_fr)\n",
        "\n",
        "wikipedia_url_ma =ma_json[1][\"wikipedia_url\"]\n",
        "wikipedia_url_fr =fr_json[1][\"wikipedia_url\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "leaders_wiki_ma = requests.get(wikipedia_url_ma)\n",
        "leaders_wiki_fr = requests.get(wikipedia_url_fr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsqjiTYr8sK"
      },
      "source": [
        "Ouch! You get the raw HTML code of the webpage. If you try to deal with it without tools, you will be there all night. Instead, use the [beautiful soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) *external* library. You will find more info about it [here](../../2.python/2.python_advanced/05.Scraping/1.beautifulsoup_basic.ipynb) and [here](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb)\n",
        "\n",
        "Using the Quickstart section, start by importing the library and loading the output of your `get_text()` function.\n",
        "\n",
        "Use the `prettify()` function and print it to take a look. You will start the actual parsing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h79ahwJvr7p-"
      },
      "outputs": [],
      "source": [
        "soup_fr = BeautifulSoup(leaders_wiki_fr.content, \"html\")\n",
        "soup_fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soup_ma = BeautifulSoup(leaders_wiki_ma.content, \"html\")\n",
        "soup_ma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsLjaig7_dY"
      },
      "source": [
        "That looks better but you need to extract the right part of the webpage: the text of the first paragraph.\n",
        "\n",
        "It is a bit tricky because Wikipedia pages slightly differ in structure from one language to the next. We cannot simply get the text for the first HTML paragraph.\n",
        "\n",
        "You will start by getting all the HTML paragraphs from the HTML source and saving them in the `paragraphs` variable.\n",
        "\n",
        "Use the documentation or google the appropriate keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs8HeBx19oyC"
      },
      "outputs": [],
      "source": [
        "# 2 lines\n",
        "for p in soup.find_all(\"p\"):\n",
        "    print(p.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tlaL3aM9zoo"
      },
      "source": [
        "If you try different urls, you might find that the paragraph you want may be at a different index each time.\n",
        "\n",
        "That is where you need to be clever and ask yourself what would be a reliable way to identify the right index ie. which string matches only the first paragraph whatever the language...\n",
        "\n",
        "Spend a good 30 minutes on the problem and brainstorm with your fellow learners. If you come out empty handed, ask your coach.\n",
        "\n",
        "1. Loop over the HTML paragraphs\n",
        "2. When you have identified the correct one:\n",
        "   * Store the [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output) inside the `first_paragraph` variable\n",
        "   * Exit the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_us = leaders_p_country['us']\n",
        "us_json = json.loads(dict_us)\n",
        "wikipedia_url_us = us_json[0][\"wikipedia_url\"]\n",
        "print(wikipedia_url_us)\n",
        "leaders_wiki_us = requests.get(wikipedia_url_us)\n",
        "soup_us = BeautifulSoup(leaders_wiki_us.content, \"html\")\n",
        "soup_us\n",
        "parrag_us=[]\n",
        "for p in soup_us.find_all(\"p\"):\n",
        "    parrag_us.append(p)\n",
        "\n",
        "for p in parrag_us:\n",
        "    # Find the first instance of bold. It should be the name\n",
        "    if p.find(\"b\"): \n",
        "        print(p.text)\n",
        "        # so it only takes the first bold instance\n",
        "        break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing with every country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_fr = leaders_p_country['fr']\n",
        "fr_json = json.loads(dict_fr)\n",
        "wikipedia_url_fr = fr_json[0][\"wikipedia_url\"]\n",
        "print(wikipedia_url_fr)\n",
        "leaders_wiki_fr = requests.get(wikipedia_url_fr)\n",
        "soup_fr = BeautifulSoup(leaders_wiki_fr.content, \"html\")\n",
        "soup_fr\n",
        "parrag_fr=[]\n",
        "for p in soup_fr.find_all(\"p\"):\n",
        "    parrag_fr.append(p)\n",
        "\n",
        "for p in parrag_fr:\n",
        "    if p.find(\"b\"):\n",
        "        print(p.text)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_be = leaders_p_country['be']\n",
        "be_json = json.loads(dict_be)\n",
        "wikipedia_url_be = be_json[0][\"wikipedia_url\"]\n",
        "print(wikipedia_url_be)\n",
        "leaders_wiki_be = requests.get(wikipedia_url_be)\n",
        "soup_be = BeautifulSoup(leaders_wiki_be.content, \"html\")\n",
        "soup_be\n",
        "parrag_be=[]\n",
        "for p in soup_be.find_all(\"p\"):\n",
        "    parrag_be.append(p)\n",
        "\n",
        "for p in parrag_be:\n",
        "    if p.find(\"b\"):\n",
        "        print(p.text)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_ma = leaders_p_country['ma']\n",
        "ma_json = json.loads(dict_ma)\n",
        "wikipedia_url_ma =ma_json[0][\"wikipedia_url\"]\n",
        "print(wikipedia_url_ma)\n",
        "leaders_wiki_ma = requests.get(wikipedia_url_ma)\n",
        "soup_ma = BeautifulSoup(leaders_wiki_ma.content, \"html\")\n",
        "soup_ma\n",
        "parrag_ma=[]\n",
        "for p in soup_ma.find_all(\"p\"):\n",
        "    parrag_ma.append(p)\n",
        "\n",
        "for p in parrag_ma:\n",
        "    if p.find(\"b\"):\n",
        "        print(p.text)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_ru = leaders_p_country['ru']\n",
        "ru_json = json.loads(dict_ru)\n",
        "wikipedia_url_ru =ru_json[0][\"wikipedia_url\"]\n",
        "print(wikipedia_url_ru)\n",
        "leaders_wiki_ru = requests.get(wikipedia_url_ru)\n",
        "soup_ru = BeautifulSoup(leaders_wiki_ru.content, \"html\")\n",
        "\n",
        "parrag_ru=[]\n",
        "for p in soup_ru.find_all(\"p\"):\n",
        "    parrag_ru.append(p)\n",
        "    \n",
        "for p in parrag_ru:\n",
        "    if p.find(\"b\"):\n",
        "        print(p.text)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-1LlIHBGm"
      },
      "source": [
        "At this stage, you can create a function to maintain consistency in your code. We will give you its *skeleton*, you will copy the code you wrote and make it work inside a function.\n",
        "\n",
        "Don't forget to test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQORoweDHARO"
      },
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "def get_first_paragraph(wikipedia_url):\n",
        "    print(wikipedia_url) # keep this for the rest of the notebook\n",
        "#   [insert your code]\n",
        "#   return first_paragraph\n",
        "\n",
        "    leaders_wiki = requests.get(wikipedia_url)\n",
        "    soup_leaders = BeautifulSoup(leaders_wiki .content, \"html\")\n",
        "   \n",
        "    parragraphs=[]\n",
        "    for p in soup_leaders.find_all(\"p\"):\n",
        "        parragraphs.append(p)\n",
        "\n",
        "    for p in parragraphs:\n",
        "        # Find the first instance of bold. It should be the name\n",
        "        if p.find(\"b\"): \n",
        "            return p.text\n",
        "            # so it only takes the first bold instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_first_paragraph(wikipedia_url_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_first_paragraph(wikipedia_url_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_first_paragraph(wikipedia_url_be)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_first_paragraph(wikipedia_url_ma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_first_paragraph(wikipedia_url_ru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtoM4dgsBVoD"
      },
      "source": [
        "### 2a. Regular expressions to the rescue\n",
        "\n",
        "Now that you have extracted the content of the first paragraph, the only thing that remains to finish your Wikipedia scraper is to sanitize the output.\n",
        "\n",
        "Indeed some Wikipedia references, HTML code, phonetic pronunciation etc. may linger. You might find *regular expressions* handy to get rid of them and obtain pristine text. You will find some useful documentation about regular expressions [here](../../2.python/2.python_advanced/03.Regex/regex.ipynb)\n",
        "\n",
        "Once you have one of your regex working online, try it in the cell below. \n",
        "\n",
        "Hints: \n",
        "* Check the `sub()` method documentation.\n",
        "* Make sure to test urls in different languages. Some may look good but other do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DHEAb6oBUxd"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "import re\n",
        "# ### things to get rid of:\n",
        "##/xa0\n",
        "## ['number']\n",
        "\n",
        "pattern = '\\S*ⓘ\\S*|/xa0|\\[.{0,3}?\\]'\n",
        "\n",
        "wiki_urls= [wikipedia_url_us, wikipedia_url_be, wikipedia_url_fr, wikipedia_url_ma, wikipedia_url_ru]\n",
        "\n",
        "for url in wiki_urls:\n",
        "    s = get_first_paragraph(url)\n",
        "    print(\"-------LEADER------\")\n",
        "    print(s)\n",
        "    \n",
        "    clean_p = re.sub(pattern,'',s)\n",
        "\n",
        "    print(clean_p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekqWs-4E0bK"
      },
      "source": [
        "Overwrite the `get_first_paragraph()` function by applying your regex to the first paragraph before returning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voT-jzd7FMOc"
      },
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "def get_first_paragraph(wikipedia_url):\n",
        "    print(wikipedia_url) # keep this for the rest of the notebook\n",
        "#   [insert your code]\n",
        "#   return first_paragraph\n",
        "\n",
        "    leaders_wiki = requests.get(wikipedia_url)\n",
        "    soup_leaders = BeautifulSoup(leaders_wiki .content, \"html\")\n",
        "   \n",
        "    parragraphs=[]\n",
        "    for p in soup_leaders.find_all(\"p\"):\n",
        "        parragraphs.append(p)\n",
        "\n",
        "    for p in parragraphs:\n",
        "        # Find the first instance of bold. It should be the name\n",
        "        if p.find(\"b\"): \n",
        "            raw_first_p = p.text\n",
        "            # so it only takes the first bold instance\n",
        "        \n",
        "    pattern = '\\S*ⓘ\\S*|/xa0|\\[.{0,3}?\\]'\n",
        "\n",
        "    clean_first_p = re.sub(pattern,'',raw_first_p)\n",
        "\n",
        "    return clean_first_p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_urls= [wikipedia_url_us, wikipedia_url_be, wikipedia_url_fr, wikipedia_url_ma, wikipedia_url_ru]\n",
        "\n",
        "for url in wiki_urls:\n",
        "    s = get_first_paragraph(url)\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZkffTEFR0z"
      },
      "source": [
        "Come up with other regexes to capture other patterns and sanitize the outputs completely. Modify your `get_first_paragraph()` function accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOmkLM9JFRCp"
      },
      "outputs": [],
      "source": [
        "# < 20 lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY7bM7Z1Nr0K"
      },
      "source": [
        "## 3. Putting it all together\n",
        "\n",
        "Let's go back to your `get_leaders()` function and update it with an *inner* loop over each leader. You will query the url provided and extract the first paragraph using the `get_first_paragraph()` function you just finished. You will then update that `leader`'s dictionary and move on to the next one.\n",
        "\n",
        "Notice, the rest of the code should not change since you modify the leader's data one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "def get_first_paragraph(wikipedia_url):\n",
        "    print(wikipedia_url) # keep this for the rest of the notebook\n",
        "\n",
        "    leaders_wiki = requests.get(wikipedia_url)\n",
        "    soup_leaders = BeautifulSoup(leaders_wiki .content, \"html\")\n",
        "   \n",
        "    parragraphs=[]\n",
        "    for p in soup_leaders.find_all(\"p\"):\n",
        "        parragraphs.append(p)\n",
        "\n",
        "    for p in parragraphs:\n",
        "        # Find the first instance of bold. It should be the name\n",
        "        if p.find(\"b\"): \n",
        "            raw_first_p = p.text\n",
        "            # so it only takes the first bold instance\n",
        "            break\n",
        "        \n",
        "    pattern = '\\S*ⓘ\\S*|/xa0|\\[.{0,3}?\\]'\n",
        "\n",
        "    clean_first_p = re.sub(pattern,'',raw_first_p)\n",
        "\n",
        "    return clean_first_p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_leaders():\n",
        "    # Establishing the urls\n",
        "    root_url = \"https://country-leaders.onrender.com/\"\n",
        "    cookie_url = root_url+\"cookie/\"\n",
        "    countries_url = root_url + \"countries\"\n",
        "    leaders_url = root_url+\"/leaders\"\n",
        "    leaders_per_country = dict()\n",
        "\n",
        "    # Getting the cookie\n",
        "    cookie_resp = requests.get(cookie_url)\n",
        "    user_cookie = cookie_resp.cookies['user_cookie']\n",
        "\n",
        "    countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "    countries = countries_resp.json()\n",
        "    \n",
        "    for country in countries:\n",
        "        country_resp = requests.get(leaders_url, cookies={'user_cookie':user_cookie}, params={'country':country})\n",
        "        print(country_resp )\n",
        "        leaders_per_country[country]=country_resp.text\n",
        "\n",
        "    all_leaders = []\n",
        "\n",
        "    # Obtaining the Wikipedia url\n",
        "    for country, leaders in leaders_per_country.items():\n",
        "        leaders_dict = json.loads(leaders)\n",
        "        for leaders in leaders_dict:\n",
        "            leaders['country'] = country\n",
        "            all_leaders.append(leaders)\n",
        "    \n",
        " \n",
        "    for leader in all_leaders:\n",
        "        wiki_parragraph = get_first_paragraph(leader['wikipedia_url'])\n",
        "        leader['wiki_parragraph'] = wiki_parragraph\n",
        "     \n",
        "               \n",
        "    return all_leaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = get_leaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete this cell"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8sCKxGrnJCxv"
      },
      "source": [
        "Does the function crash in the middle of the loop? Chances are the cookies have expired while looping over the leaders.\n",
        "\n",
        "Modify your function with an *exception* or check if the `status_code` is a cookie error. In either case, get new cookies and query the api again.\n",
        "\n",
        "If your code did not crash,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgPd2dxgJiW1"
      },
      "outputs": [],
      "source": [
        "# < 25 lines\n",
        "def get_leaders():\n",
        "    # Establishing the urls\n",
        "    root_url = \"https://country-leaders.onrender.com/\"\n",
        "    cookie_url = root_url+\"cookie/\"\n",
        "    countries_url = root_url + \"countries\"\n",
        "    leaders_url = root_url+\"/leaders\"\n",
        "    leaders_per_country = dict()\n",
        "\n",
        "    # Getting the cookie\n",
        "    cookie_resp = requests.get(cookie_url)\n",
        "    user_cookie = cookie_resp.cookies['user_cookie']\n",
        "\n",
        "    countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "\n",
        "    if cookie_resp.status_code != 200:\n",
        "        cookie_resp = requests.get(cookie_url)\n",
        "        user_cookie = cookie_resp.cookies['user_cookie']\n",
        "        countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "        \n",
        "    countries = countries_resp.json()\n",
        "    \n",
        "    for country in countries:\n",
        "        country_resp = requests.get(leaders_url, cookies={'user_cookie':user_cookie}, params={'country':country})\n",
        "        print(country_resp )\n",
        "        leaders_per_country[country]=country_resp.text\n",
        "\n",
        "    all_leaders = []\n",
        "\n",
        "    # Obtaining the Wikipedia url\n",
        "    for country, leaders in leaders_per_country.items():\n",
        "        leaders_dict = json.loads(leaders)\n",
        "        for leaders in leaders_dict:\n",
        "            leaders['country'] = country\n",
        "            all_leaders.append(leaders)\n",
        "\n",
        "    for leader in all_leaders:\n",
        "        wiki_parragraph = get_first_paragraph(leader['wikipedia_url'])\n",
        "        leader['wiki_parragraph'] = wiki_parragraph\n",
        "     \n",
        "               \n",
        "    return all_leaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvV-kPsKLl0"
      },
      "source": [
        "Check the output of your function again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPXT-cxgKQof"
      },
      "outputs": [],
      "source": [
        "# Check the output of your function (1 line)\n",
        "get_leaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9lgxh_NMSO3"
      },
      "source": [
        "Well done! It took a while however... Let's speed things up. The main *bottleneck* is the loop. We call on the Wikipedia website many times.\n",
        "\n",
        "You will use the same *session* to call all the wikipedia pages. Check the *Advanced Usage* section of the Requests module's documentation.\n",
        "\n",
        "Start by modifying the `get_first_paragraph()` function to accept a session parameter and adjust the `get()` method call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTEVLIJ-V7z1"
      },
      "outputs": [],
      "source": [
        "# < 20 lines\n",
        "def get_first_paragraph(wikipedia_url, session):\n",
        "    print(wikipedia_url) # keep this for the rest of the notebook\n",
        "    \n",
        "    leaders_wiki = session.get(wikipedia_url)\n",
        "    soup_leaders = BeautifulSoup(leaders_wiki.content, \"html\")\n",
        "   \n",
        "    parragraphs=[]\n",
        "    for p in soup_leaders.find_all(\"p\"):\n",
        "        parragraphs.append(p)\n",
        "\n",
        "    for p in parragraphs:\n",
        "        # Find the first instance of bold. It should be the name\n",
        "        if p.find(\"b\"): \n",
        "            raw_first_p = p.text\n",
        "            # so it only takes the first bold instance\n",
        "            break\n",
        "    \n",
        "    #Clean the text with regex\n",
        "        \n",
        "    pattern = '\\\\n|\\S*ⓘ\\S*|xa0|\\[.{0,3}?\\]|/'\n",
        "\n",
        "    clean_first_p = re.sub(pattern,'',raw_first_p)\n",
        "\n",
        "    return clean_first_p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gcgHMkeWK5-"
      },
      "source": [
        "Modify your `get_leaders()` function to make use of a single session for all the Wikipedia calls.\n",
        "1. create a `Session` object outside of the loop over countries.\n",
        "2. pass it to the `get_first_paragraph()` function as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt6JtKDpOAIe"
      },
      "outputs": [],
      "source": [
        "# <25 lines\n",
        "def get_leaders():\n",
        "    # Establishing the urls\n",
        "    root_url = \"https://country-leaders.onrender.com/\"\n",
        "    cookie_url = root_url+\"cookie/\"\n",
        "    countries_url = root_url + \"countries\"\n",
        "    leaders_url = root_url+\"/leaders\"\n",
        "    leaders_per_country = dict()\n",
        "\n",
        "    # Getting the cookie\n",
        "    cookie_resp = requests.get(cookie_url)\n",
        "    user_cookie = cookie_resp.cookies['user_cookie']\n",
        "    \n",
        "    countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "\n",
        "    if cookie_resp.status_code != 200:\n",
        "        cookie_resp = requests.get(cookie_url)\n",
        "        user_cookie = cookie_resp.cookies['user_cookie']\n",
        "        countries_resp = requests.get(countries_url, cookies={'user_cookie':user_cookie})\n",
        "        \n",
        "    countries = countries_resp.json()\n",
        "    \n",
        "    for country in countries:\n",
        "        country_resp = requests.get(leaders_url, cookies={'user_cookie':user_cookie}, params={'country':country})\n",
        "        print(country_resp )\n",
        "        leaders_per_country[country]=country_resp.text\n",
        "\n",
        "    all_leaders =[]\n",
        "\n",
        "    # Obtaining the Wikipedia url\n",
        "    for country, leaders in leaders_per_country.items():\n",
        "        leaders_dict = json.loads(leaders)\n",
        "        for leaders in leaders_dict:\n",
        "            leaders['country'] = country\n",
        "            all_leaders.append(leaders)\n",
        "\n",
        "    #s= requests.Session()\n",
        "\n",
        "    #for leader in all_leaders:\n",
        "        #wiki_parragraph = get_first_paragraph(leader['wikipedia_url'], s)\n",
        "        #leader['wiki_parragraph'] = wiki_parragraph\n",
        "    \n",
        "    with requests.Session() as s:\n",
        "        for leader in all_leaders:\n",
        "            wiki_parragraph = get_first_paragraph(leader['wikipedia_url'], s)\n",
        "            leader['wiki_parragraph'] = wiki_parragraph\n",
        "        \n",
        "               \n",
        "    return all_leaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy09nMG9VOaI"
      },
      "source": [
        "Test your new functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5wFE6ivRf-Z"
      },
      "outputs": [],
      "source": [
        "leaders_one_session = get_leaders()\n",
        "print(leaders_one_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(leaders_one_session)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LtLTvw3wPqe0"
      },
      "source": [
        "## 4. Saving your hard work\n",
        "\n",
        "The final step is to save the ``leaders_per_country`` dictionary in the `leaders.json` file using the [json](https://docs.python.org/3/library/json.html) module. Check out the `with` statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTNGKKrOjNDk"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "json_leaders = json.dumps(leaders_one_session, ensure_ascii=False, indent =4)\n",
        "\n",
        "with open(\"leaders.json\", \"w\", encoding='utf-8') as outfile:\n",
        "    outfile.write(json_leaders)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uf_kfGCWmM"
      },
      "source": [
        "Make sure the file can be read back. Write the code to read the file. And check the variables are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VwNjBYyjPzs"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "with open (\"leaders.json\", \"r\", encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fW_U7gXktyv"
      },
      "source": [
        "Make a function `save(leaders_per_country)` to call this code easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfknpnTljqUd"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "def save(leaders_per_country):\n",
        "    json_leaders = json.dumps(leaders_per_country, ensure_ascii=False, indent =4)\n",
        "\n",
        "    with open(\"leaders.json\", \"w\", encoding='utf-8') as outfile:\n",
        "        outfile.write(json_leaders)\n",
        "    \n",
        "    with open (\"leaders.json\", \"r\", encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    print(\"The following content has been saved to 'leaders.json' :\")\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWQ6bbn31cix"
      },
      "outputs": [],
      "source": [
        "# Call the function (1 line)\n",
        "save(leaders_one_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5W6nPihlQo9"
      },
      "source": [
        "## 5. Tidy things up in a stand-alone python script\n",
        "\n",
        "Congratulations! You now have a working scraper! However, your code is scattered throughout this notebook along side the tutorials. Hardly production ready...\n",
        "\n",
        "Copy and paste what you need in a separate `leaders_scraper.py` file.\n",
        "Make sure it works by calling `python3 leaders_scraper.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0cvv193mlxY"
      },
      "source": [
        "## (Optional) To go further\n",
        "\n",
        "If you want to practice scraping, you can read this section and tackle the exercises.\n",
        "\n",
        "1. Restructure your code by using OOP (see ReadMe).\n",
        "2. You have noticed the API returns very partial results for country leaders. Many are missing. Overwrite the `get_leaders()` function to get its list from Wikipedia and extract their *personal details* from the frame on the side.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Missing personal details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "leaders_data = get_leaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Identifying which keys have missing information\n",
        "\n",
        "for leader in leaders_data:\n",
        "    for key, value in leader.items():\n",
        "        if not value:\n",
        "            print(f\"Leader ID: {leader['id']}: {key} : {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_data_leaders = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_missing_info(leader_data, session): \n",
        "    print(leader_data['wikipedia_url'])\n",
        "    for key, value in leader_data.items():\n",
        "        if not value:\n",
        "            value_to_find = key\n",
        "            \n",
        "    \n",
        "    leader_wiki = session.get(leader_data['wikipedia_url'])\n",
        "\n",
        "    soup_leader = BeautifulSoup(leader_wiki.content, \"html\")\n",
        "\n",
        "    text = \"\"\n",
        "    \n",
        "    \n",
        "    # For English text:\n",
        "    if value_to_find == 'birth_date':\n",
        "        #find the birth data in the soup\n",
        "        text ='Born'\n",
        "        key_to_fill = value_to_find\n",
        "        \n",
        "    elif value_to_find == 'death_date':\n",
        "        # find the death data in the soup    \n",
        "        text = 'Died'\n",
        "        key_to_fill = value_to_find\n",
        "    \n",
        "    else:\n",
        "        return\n",
        "\n",
        "    for i in soup_leader.find_all('th', text=text):\n",
        "        print(\"Found!\")\n",
        "        print(\"-------------\")\n",
        "        missing_raw_info = i.find_next().text\n",
        "        print(missing_raw_info)\n",
        "        date_pattern = r'([A-Z][a-z]+) \\s*(\\d{1,2}),\\s*(\\d{4})'\n",
        "        match = re.search(date_pattern, missing_raw_info)\n",
        "        print(match.group())\n",
        "        \n",
        "        date_obj = datetime.strptime(match.group(), \"%B %d, %Y\").date()\n",
        "        \n",
        "        print(date_obj, type(date_obj))\n",
        "        leader_data[key_to_fill] = str(date_obj)\n",
        "    return leader_data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Joe_Biden\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m leader\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value:\n\u001b[1;32m----> 7\u001b[0m         \u001b[43mget_missing_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleader\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[46], line 10\u001b[0m, in \u001b[0;36mget_missing_info\u001b[1;34m(leader_data, session)\u001b[0m\n\u001b[0;32m      5\u001b[0m         value_to_find \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m      8\u001b[0m leader_wiki \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(leader_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikipedia_url\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m soup_leader \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleader_wiki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# For English text:\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\__init__.py:473\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\__init__.py:658\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:467\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    464\u001b[0m parser \u001b[38;5;241m=\u001b[39m BeautifulSoupHTMLParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\html\\parser.py:344\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:182\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     sourceline \u001b[38;5;241m=\u001b[39m sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\__init__.py:1032\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m# Assume that this is either Tag or a subclass of Tag. If not,\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# the user brought type-unsafety upon themselves.\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m tag_class \u001b[38;5;241m=\u001b[39m cast(Type[Tag], tag_class)\n\u001b[1;32m-> 1032\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[43mtag_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentTag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_most_recent_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourceline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msourcepos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
            "File \u001b[1;32mc:\\Users\\herms\\Desktop\\BeCode-Bauman\\BXL-Bouman-9\\projects\\wikipedia-scraper\\wikipedia-venv\\lib\\site-packages\\bs4\\element.py:1671\u001b[0m, in \u001b[0;36mTag.__init__\u001b[1;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[0;32m   1669\u001b[0m attr_dict_class: \u001b[38;5;28mtype\u001b[39m[AttributeDict]\n\u001b[0;32m   1670\u001b[0m attribute_value_list_class: \u001b[38;5;28mtype\u001b[39m[AttributeValueList]\n\u001b[1;32m-> 1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_xml:\n\u001b[0;32m   1673\u001b[0m         attr_dict_class \u001b[38;5;241m=\u001b[39m XMLAttributeDict\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "s = requests.Session()\n",
        "\n",
        "for leader in leaders_data:\n",
        "    for key, value in leader.items():\n",
        "        if not value:\n",
        "        \n",
        "            get_missing_info(leader,s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "wikipedia-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
